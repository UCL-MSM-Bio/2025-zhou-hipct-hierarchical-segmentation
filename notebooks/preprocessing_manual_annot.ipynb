{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data to CLAHE\n",
    "The pre-process steps for HiP-CT 16bit data are as follows:\n",
    "* 3D CLAHE applied to the 16 bit data \n",
    "* Normalisation to 8 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "import numpy as np\n",
    "import skimage.io as skio\n",
    "from segmentation.preprocessing import preprocessor\n",
    "import segmentation.preprocessing.helper as helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cubes: 40\n",
      "cube shape: (512, 512, 512)\n",
      "cube dtype: uint16\n"
     ]
    }
   ],
   "source": [
    "# high-resolution 2.58um - 5.2um data conversion\n",
    "input_folder = '/hdd/yang/data/kidney_seg/publish_data/highres_training_16bit_cubes/'\n",
    "save_folder = '/hdd/yang/data/kidney_seg/high-res_training/highres_training_8bit_clahe/'\n",
    "cube_paths = natsort.natsorted(glob.glob(os.path.join(input_folder, '*.tif')))\n",
    "print(f'number of cubes: {len(cube_paths)}')\n",
    "skio.imread(cube_paths[0])  # check if the first cube can be read\n",
    "print(f'cube shape: {skio.imread(cube_paths[0]).shape}')\n",
    "print(f'cube dtype: {skio.imread(cube_paths[0]).dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 40 \n",
      " File type: uint16 \n",
      " Converting to 8 bit: True \n",
      " Masked: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_test_split.txt\n",
    "hipct_clahe.clahe_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating training patches\n",
    "The cubes of 512^3 pixels after CLAHE and 8 bit conversion will be devided into 128^3 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cube files:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cube file: complete_cube_39.tif: 100%|██████████| 40/40 [00:23<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed splitting the data patches!\n",
      "Number of label files:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing label file: complete_label_39.tif: 100%|██████████| 40/40 [00:24<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed splitting the label patches!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_dir = save_folder \n",
    "label_dir = '/hdd/yang/data/kidney_seg/publish_data/highres_training_16bit_labels' \n",
    "save_dir = '/hdd/yang/data/kidney_seg/high-res_training/training_patches'\n",
    "split_size = 128\n",
    "\n",
    "preprocessor.generate_training_patches(\n",
    "    image_dir=image_dir,\n",
    "    label_dir=label_dir,\n",
    "    save_dir=save_dir,\n",
    "    split_size=split_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training and testing patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 5um S-20-28, Total Cubes: 20\n",
      "Train Indices: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19], total: 18\n",
      "Test Indices: [7, 13], total: 2\n",
      "Sample: 5.2um LADAF 2021-17 Left Kidney, Total Cubes: 7\n",
      "Train Indices: [20, 21, 22, 23, 24, 25], total: 6\n",
      "Test Indices: [26], total: 1\n",
      "Sample: 5.2um LADAF 2021-17 Right Kidney, Total Cubes: 9\n",
      "Train Indices: [27, 29, 30, 31, 32, 33, 34, 35], total: 8\n",
      "Test Indices: [28], total: 1\n",
      "Sample: 2.58um LADAF 2020-27 Left Kidney, Total Cubes: 4\n",
      "Train Indices: [37, 38, 39], total: 3\n",
      "Test Indices: [36], total: 1\n"
     ]
    }
   ],
   "source": [
    "# select the training and testing cubes across each sample\n",
    "helper.train_test_split(\n",
    "    save_dir='../data/high-res_training/90-10',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sample: 5um S-20-28\n",
      "Total number of valid cubes: 950\n",
      "Working on sample: 5.2um LADAF 2021-17 Left Kidney\n",
      "Total number of valid cubes: 197\n",
      "Working on sample: 5.2um LADAF 2021-17 Right Kidney\n",
      "Total number of valid cubes: 239\n",
      "Working on sample: 2.58um LADAF 2020-27 Left Kidney\n",
      "Total number of valid cubes: 112\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Generate 5-fold cross-validation splits\n",
    "helper.generate_patch_list_per_sample(\n",
    "    label_dir='../data/high-res_training/labels',\n",
    "    train_cubes_txt='../data/high-res_training/90-10/train_selected_cubes_tr0.9.txt',\n",
    "    test_cube_txt='../data/high-res_training/90-10/test_selected_cubes_tr0.9.txt',\n",
    "    output_path='../data/high-res_training/90-10'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sample: 5um S-20-28\n",
      "Train: 706\n",
      "Val: 177\n",
      "Train: 706\n",
      "Val: 177\n",
      "Train: 706\n",
      "Val: 177\n",
      "Train: 707\n",
      "Val: 176\n",
      "Train: 707\n",
      "Val: 176\n",
      "Working on sample: 5.2um LADAF 2021-17 Left Kidney\n",
      "Train: 117\n",
      "Val: 30\n",
      "Train: 117\n",
      "Val: 30\n",
      "Train: 118\n",
      "Val: 29\n",
      "Train: 118\n",
      "Val: 29\n",
      "Train: 118\n",
      "Val: 29\n",
      "Working on sample: 5.2um LADAF 2021-17 Right Kidney\n",
      "Train: 164\n",
      "Val: 41\n",
      "Train: 164\n",
      "Val: 41\n",
      "Train: 164\n",
      "Val: 41\n",
      "Train: 164\n",
      "Val: 41\n",
      "Train: 164\n",
      "Val: 41\n",
      "Working on sample: 2.58um LADAF 2020-27 Left Kidney\n",
      "Train: 63\n",
      "Val: 16\n",
      "Train: 63\n",
      "Val: 16\n",
      "Train: 63\n",
      "Val: 16\n",
      "Train: 63\n",
      "Val: 16\n",
      "Train: 64\n",
      "Val: 15\n"
     ]
    }
   ],
   "source": [
    "helper.generate_folds(\n",
    "    n_folds=5, \n",
    "    patch_list_per_sample='../data/high-res_training/90-10/non_zero_patch_list_per_sample.json', \n",
    "    output_json_dir='../data/high-res_training/90-10'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying training data:   0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying training data: 100%|██████████| 1314/1314 [00:03<00:00, 382.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we can prepare the training dataset for model training from the json files\n",
    "helper.processing_nnunet_dataset(\n",
    "    patch_dir='../data/high-res_training',\n",
    "    patch_list_per_sample='../data/high-res_training/90-10/non_zero_patch_list_per_sample.json',\n",
    "    nnunet_raw_data_dir='../data/nnUNet_raw/',\n",
    "    dataset_name='Dataset001_Glomeruli'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
